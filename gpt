#!/usr/bin/env python3

import argparse
import dataclasses
import datetime
import json
import logging
import os
import sys
from enum import Enum

import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

LOG = logging.getLogger(__name__)


class Model(Enum):
    """
    The name of the OpenAI model to use for the request. This is a
    required parameter, and you'll need to choose the model that's best
    suited to your use case.
    """

    TEXT_DAVINCI_003 = "text-davinci-003"
    """
    Most capable GPT-3 model. Can do any task the other models can do, often
    with higher quality, longer output and better instruction-following. Also
    supports inserting completions within text.
    """

    CODE_DAVINCI_002 = "code-davinci-002"
    """
    Most capable Codex model. Particularly good at translating natural
    language to code. In addition to completing code, also supports inserting
    completions within code.
    """

    CODE_CUSHMAN_001 = "code-cushman-001"
    """
    Almost as capable as Davinci Codex, but slightly faster. This speed
    advantage may make it preferable for real-time applications.
    """


@dataclasses.dataclass
class Completion:
    text: str


STOP = "\n"
DEFAULT_MODEL = Model.TEXT_DAVINCI_003
DEFAULT_TEMPERATURE = 0
DEFAULT_MAX_TOKENS = 256
DEFAULT_TOP_P = 1.0
DEFAULT_FREQUENCY_PENALTY = 0.0
DEFAULT_PRESENCE_PENALTY = 0.0


def to_json(obj, indent=None):
    """
    Convert an object to JSON string.

    :param obj: The object to convert.
    :param indent: The number of spaces to indent the JSON string.

    :return: The JSON string.
    """

    class CustomJSONEncoder(json.JSONEncoder):
        def default(self, obj):
            if dataclasses.is_dataclass(obj):
                return dataclasses.asdict(obj)
            if isinstance(obj, Enum):
                return obj.value
            if isinstance(obj, (datetime.datetime, datetime.date)):
                return obj.isoformat()
            return super().default(obj)

    return json.dumps(obj, cls=CustomJSONEncoder, indent=indent)


def create_completion(
    model: Model,
    prompt: str,
    temperature: float,
    max_tokens: int,
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
) -> Completion:
    """
    Create a completion request.

    :param model: The name of the OpenAI model to use for the request.
    :param prompt: The prompt or text to use as input for the model.
    :param temperature: A float value that controls the creativity of the generated output.
    :param max_tokens: An integer value that controls the maximum number of tokens (words, punctuation, and other symbols) to generate in response to the prompt.
    :param top_p: A float value that controls the diversity of the generated output.
    :param frequency_penalty: A float value that discourages the model from generating text that includes certain keywords or phrases.
    :param presence_penalty: A float value that encourages the model to generate text that includes certain keywords or phrases.

    :return: A completion request.
    """
    request = dict(
        model=model.value,
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
    )
    LOG.debug("request:")
    LOG.debug(f"json```\n{to_json(request, indent=2)}```")

    response = openai.Completion.create(**request)

    LOG.debug("response:")
    LOG.debug(f"json```\n{to_json(response, indent=2)}```")
    return Completion(response.choices[0].text)


def main():
    parser = argparse.ArgumentParser(description="OpenAI GPT-3 text completion")
    parser.add_argument(
        "file",
        nargs="?",
        type=argparse.FileType("r"),
        default=sys.stdin,
        help="input file",
    )
    parser.add_argument("chat", action="store_true", help="chat mode")
    parser.add_argument("--debug", action="store_true", help="enable debug output")
    parser.add_argument(
        "--temperature",
        type=float,
        default=DEFAULT_TEMPERATURE,
        help=f"temperature (default: {DEFAULT_TEMPERATURE})",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=DEFAULT_MAX_TOKENS,
        help=f"max tokens (default: {DEFAULT_MAX_TOKENS})",
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=DEFAULT_TOP_P,
        help=f"top p (default: {DEFAULT_TOP_P})",
    )
    parser.add_argument(
        "--frequency-penalty",
        type=float,
        default=DEFAULT_FREQUENCY_PENALTY,
        help=f"frequency penalty (default: {DEFAULT_FREQUENCY_PENALTY})",
    )
    parser.add_argument(
        "--presence-penalty",
        type=float,
        default=DEFAULT_PRESENCE_PENALTY,
        help=f"presence penalty (default: {DEFAULT_PRESENCE_PENALTY})",
    )
    parser.add_argument(
        "--model",
        type=Model,
        default=DEFAULT_MODEL,
        help=f"model (default: {DEFAULT_MODEL})",
    )
    parser.add_argument(
        "--default",
        action="store_true",
        help="print default parameters",
    )
    parser.add_argument(
        "--silent",
        action="store_true",
        help="silent, no print of input",
    )

    args = parser.parse_args()

    if args.default:
        defaults = {
            "temperature": DEFAULT_TEMPERATURE,
            "max_tokens": DEFAULT_MAX_TOKENS,
            "top_p": DEFAULT_TOP_P,
            "frequency_penalty": DEFAULT_FREQUENCY_PENALTY,
            "presence_penalty": DEFAULT_PRESENCE_PENALTY,
            "model": DEFAULT_MODEL,
        }
        print(to_json(defaults, indent=2))
        return

    if args.debug:
        LOG.setLevel(logging.DEBUG)
        LOG.debug("Verbose output enabled")
        LOG.debug("argv: %s", sys.argv)

    if args.chat:
        history = ""
        while True:
            prompt = input("> ")
            if prompt == "exit":
                break
            history += prompt
            completion = create_completion(
                model=args.model,
                prompt=history + STOP,
                temperature=args.temperature,
                max_tokens=args.max_tokens,
                top_p=args.top_p,
                frequency_penalty=args.frequency_penalty,
                presence_penalty=args.presence_penalty,
            )
            history += completion.text + STOP
            print(completion.text)


    input_text = args.file.read().strip()
    LOG.debug("input: %s", input_text)

    if not args.silent:
        print(input_text)

    completion = create_completion(
        model=args.model,
        prompt=f"{input_text}{STOP}",
        temperature=args.temperature,
        max_tokens=args.max_tokens,
        top_p=args.top_p,
        frequency_penalty=args.frequency_penalty,
        presence_penalty=args.presence_penalty,
    )
    print(completion.text)


if __name__ == "__main__":
    try:
        logging.basicConfig(
            stream=sys.stdout,
            level=logging.INFO,
            format="%(message)s",
        )
        main()
    except KeyboardInterrupt:
        LOG.debug("KeyboardInterrupt")
        sys.exit(1)
